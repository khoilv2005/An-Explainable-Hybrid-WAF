# -*- coding: utf-8 -*-
"""
üîç WAF EXPLAINER - PHI√äN B·∫¢N ƒê√É FIX
=====================================
C√ÅC C·∫¢I TI·∫æN:
1. ‚úÖ FIX: LIME char-level kh·ªõp v·ªõi model tokenization
2. ‚úÖ Th√™m explanation cho c·∫£ sequence (kh√¥ng ch·ªâ t·ª´ng k√Ω t·ª±)
3. ‚úÖ Highlight nguy hi·ªÉm patterns (SQL keywords, XSS tags)
4. ‚úÖ Better visualization v·ªõi m√†u s·∫Øc
5. ‚úÖ Export HTML explanation
"""

import torch
import pickle
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
from lime.lime_text import LimeTextExplainer
from model import WAF_Attention_Model
import re
from collections import defaultdict

# ==============================================================================
# C·∫§U H√åNH
# ==============================================================================
MODEL_PATH = "./data/waf_model.pth"
TOKENIZER_PATH = "./data/tokenizer.pkl"
MAX_LEN = 500
EMBEDDING_DIM = 128
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Dangerous patterns ƒë·ªÉ highlight
DANGEROUS_PATTERNS = {
    'SQL': ['select', 'union', 'drop', 'insert', 'delete', 'update', 'exec', 'execute', 
            'waitfor', 'sleep', 'benchmark', 'information_schema', 'xp_cmdshell',
            'or 1=1', 'or 0=0', "' or '", '" or "'],
    'XSS': ['<script', '</script', 'javascript:', 'onerror=', 'onload=', 'onclick=',
            'alert(', 'eval(', 'document.cookie', '<iframe', '<svg', '<img'],
    'CMD': ['etc/passwd', 'etc/shadow', 'win.ini', 'whoami', 'cat ', 'ls ', 
            'rm -rf', 'ping ', 'curl ', 'wget ', '$(', '`', '&&', '||', ';', '|'],
}

# ==============================================================================
# LOAD H·ªÜ TH·ªêNG
# ==============================================================================
def load_system():
    """Load model v√† tokenizer"""
    print("‚è≥ Loading model v√† tokenizer...")
    with open(TOKENIZER_PATH, 'rb') as f:
        tokenizer = pickle.load(f)
    vocab_size = len(tokenizer.word_index) + 1
    
    model = WAF_Attention_Model(
        vocab_size=vocab_size, 
        embedding_dim=EMBEDDING_DIM, 
        num_classes=1
    )
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    model.to(DEVICE)
    model.eval()
    
    print(f"‚úÖ Model loaded! Vocab size: {vocab_size}")
    return tokenizer, model

tokenizer, model = load_system()

# ==============================================================================
# PREDICTION WRAPPER CHO LIME
# ==============================================================================
def predict_proba(texts):
    """
    Wrapper function cho LIME
    
    L∆ØU √ù QUAN TR·ªåNG:
    - LIME s·∫Ω g·ª≠i v√†o c√°c chu·ªói ƒë√£ b·ªã perturb (x√≥a b·ªõt k√Ω t·ª±)
    - Model v·∫´n x·ª≠ l√Ω char-level tokenization nh∆∞ b√¨nh th∆∞·ªùng
    - ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o LIME explanation ch√≠nh x√°c
    """
    # Tokenize t·ª´ng k√Ω t·ª± (char-level)
    seqs = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(seqs, maxlen=MAX_LEN, padding='post', truncating='post')
    tensor = torch.LongTensor(padded).to(DEVICE)
    
    with torch.no_grad():
        outputs = model(tensor)
        probs = outputs.cpu().numpy()
    
    # Chuy·ªÉn sang format [P(normal), P(attack)] cho LIME
    results = []
    for p in probs:
        p_attack = p[0]
        p_normal = 1 - p_attack
        results.append([p_normal, p_attack])
    
    return np.array(results)

# ==============================================================================
# CHARACTER-LEVEL LIME EXPLAINER
# ==============================================================================
def create_char_level_explainer():
    """
    T·∫°o LIME explainer cho character-level
    
    ‚ö†Ô∏è QUAN TR·ªåNG: char_level=True
    ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o LIME perturb t·ª´ng k√Ω t·ª±, kh·ªõp v·ªõi c√°ch model tokenize
    """
    explainer = LimeTextExplainer(
        class_names=["B√¨nh th∆∞·ªùng", "T·∫•n c√¥ng"],
        char_level=True,  # ‚úÖ FIX: Ph·∫£i l√† True ƒë·ªÉ kh·ªõp v·ªõi model!
        split_expression=lambda x: list(x),  # Split th√†nh t·ª´ng k√Ω t·ª±
        bow=False  # Kh√¥ng d√πng Bag-of-Words v√¨ character-level
    )
    return explainer

# ==============================================================================
# PATTERN DETECTION
# ==============================================================================
def detect_dangerous_patterns(payload):
    """Ph√°t hi·ªán c√°c pattern nguy hi·ªÉm trong payload"""
    payload_lower = payload.lower()
    detected = defaultdict(list)
    
    for category, patterns in DANGEROUS_PATTERNS.items():
        for pattern in patterns:
            if pattern.lower() in payload_lower:
                detected[category].append(pattern)
    
    return detected

# ==============================================================================
# VISUALIZATION
# ==============================================================================
def colorize_text(text, weights_dict):
    """
    T√¥ m√†u text d·ª±a tr√™n importance weights
    
    M√†u ƒë·ªè: Nguy hi·ªÉm (weight > 0)
    M√†u xanh: An to√†n (weight < 0)
    M√†u tr·∫Øng: Trung t√≠nh (weight ‚âà 0)
    """
    colored_parts = []
    
    for char in text:
        weight = weights_dict.get(char, 0)
        
        if weight > 0.01:  # Nguy hi·ªÉm
            intensity = min(int(abs(weight) * 255), 255)
            colored_parts.append(f"\033[91m{char}\033[0m")  # Red
        elif weight < -0.01:  # An to√†n
            intensity = min(int(abs(weight) * 255), 255)
            colored_parts.append(f"\033[92m{char}\033[0m")  # Green
        else:  # Trung t√≠nh
            colored_parts.append(char)
    
    return ''.join(colored_parts)

def print_explanation_summary(exp, payload, prediction_proba):
    """In t√≥m t·∫Øt explanation ƒë·∫πp"""
    print("\n" + "="*70)
    print("üîç PH√ÇN T√çCH PAYLOAD")
    print("="*70)
    
    # 1. Payload g·ªëc
    print(f"\nüìù Payload:")
    print(f"   {payload}")
    
    # 2. D·ª± ƒëo√°n
    p_normal, p_attack = prediction_proba
    print(f"\nüìä D·ª± ƒëo√°n c·ªßa Model:")
    print(f"   üü¢ B√¨nh th∆∞·ªùng: {p_normal:.2%}")
    print(f"   üî¥ T·∫•n c√¥ng:    {p_attack:.2%}")
    
    verdict = "‚ö†Ô∏è  NGUY HI·ªÇM" if p_attack > 0.5 else "‚úÖ AN TO√ÄN"
    confidence = max(p_normal, p_attack)
    print(f"   {verdict} (Confidence: {confidence:.2%})")
    
    # 3. Ph√°t hi·ªán patterns
    detected = detect_dangerous_patterns(payload)
    if detected:
        print(f"\nüö® Ph√°t hi·ªán Pattern Nguy hi·ªÉm:")
        for category, patterns in detected.items():
            print(f"   [{category}] {', '.join(patterns)}")
    
    # 4. Top important characters
    print(f"\nüí° Top 15 K√Ω t·ª± Quan tr·ªçng nh·∫•t:")
    print("   " + "-"*60)
    
    char_weights = {}
    for char, weight in exp.as_list():
        if char in char_weights:
            char_weights[char] += weight
        else:
            char_weights[char] = weight
    
    # Sort by absolute weight
    sorted_chars = sorted(char_weights.items(), key=lambda x: abs(x[1]), reverse=True)
    
    for i, (char, weight) in enumerate(sorted_chars[:15], 1):
        status = "üî¥ Nguy hi·ªÉm" if weight > 0 else "üü¢ An to√†n"
        char_display = repr(char) if char in [' ', '\t', '\n'] else f"'{char}'"
        print(f"   {i:2d}. {char_display:6s} | Weight: {weight:+.4f} ({status})")
    
    # 5. Colored visualization
    print(f"\nüé® Visualization (ƒê·ªè=Nguy hi·ªÉm, Xanh=An to√†n):")
    colored = colorize_text(payload, char_weights)
    print(f"   {colored}")
    
    print("="*70)

# ==============================================================================
# NGRAM ANALYSIS (Ph√¢n t√≠ch theo c·ª•m k√Ω t·ª±)
# ==============================================================================
def analyze_ngrams(payload, exp, n=3):
    """
    Ph√¢n t√≠ch importance theo n-grams (c·ª•m k√Ω t·ª±)
    
    V√≠ d·ª•: "SELECT" c√≥ th·ªÉ ƒë∆∞·ª£c ph√¢n t√≠ch th√†nh:
    - 3-grams: "SEL", "ELE", "LEC", "ECT"
    - T·ªïng h·ª£p ƒë·ªÉ hi·ªÉu c·∫£ c·ª•m "SELECT" nguy hi·ªÉm
    """
    char_weights = {}
    for char, weight in exp.as_list():
        char_weights[char] = weight
    
    # T·∫°o n-grams
    ngrams_weights = {}
    for i in range(len(payload) - n + 1):
        ngram = payload[i:i+n]
        # T√≠nh t·ªïng weight c·ªßa c√°c k√Ω t·ª± trong ngram
        weight = sum(char_weights.get(c, 0) for c in ngram)
        ngrams_weights[ngram] = weight
    
    # Sort by absolute weight
    sorted_ngrams = sorted(ngrams_weights.items(), key=lambda x: abs(x[1]), reverse=True)
    
    print(f"\nüîé Top 10 C·ª•m {n}-k√Ω t·ª± Nguy hi·ªÉm nh·∫•t:")
    print("   " + "-"*60)
    for i, (ngram, weight) in enumerate(sorted_ngrams[:10], 1):
        status = "üî¥ Attack" if weight > 0 else "üü¢ Normal"
        print(f"   {i:2d}. '{ngram}' | Weight: {weight:+.4f} ({status})")

# ==============================================================================
# HTML EXPORT
# ==============================================================================
def export_html_explanation(exp, payload, prediction_proba, filename="explanation.html"):
    """Export explanation ra file HTML ƒë·ªÉ xem trong browser"""
    p_normal, p_attack = prediction_proba
    
    # T·∫°o HTML v·ªõi highlighting
    char_weights = {}
    for char, weight in exp.as_list():
        if char in char_weights:
            char_weights[char] += weight
        else:
            char_weights[char] = weight
    
    html_parts = []
    for char in payload:
        weight = char_weights.get(char, 0)
        
        if weight > 0.01:  # Nguy hi·ªÉm
            intensity = min(int(abs(weight) * 200) + 55, 255)
            color = f"rgb({intensity}, 0, 0)"
            html_parts.append(f'<span style="background-color: {color}; color: white; padding: 2px;">{char}</span>')
        elif weight < -0.01:  # An to√†n
            intensity = min(int(abs(weight) * 200) + 55, 255)
            color = f"rgb(0, {intensity}, 0)"
            html_parts.append(f'<span style="background-color: {color}; color: white; padding: 2px;">{char}</span>')
        else:
            html_parts.append(char)
    
    highlighted_payload = ''.join(html_parts)
    
    # T·∫°o HTML document
    html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>WAF Explanation</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}
            .container {{ background: white; padding: 20px; border-radius: 8px; max-width: 900px; margin: auto; }}
            h1 {{ color: #333; }}
            .payload {{ background: #f0f0f0; padding: 15px; border-radius: 5px; font-family: monospace; word-wrap: break-word; }}
            .prediction {{ margin: 20px 0; }}
            .bar {{ height: 30px; background: #4CAF50; border-radius: 5px; text-align: center; line-height: 30px; color: white; }}
            .bar.attack {{ background: #f44336; }}
            .legend {{ margin: 20px 0; }}
            .legend span {{ display: inline-block; padding: 5px 10px; margin: 5px; border-radius: 3px; }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>üîç WAF Explanation Report</h1>
            
            <h2>üìù Payload</h2>
            <div class="payload">{highlighted_payload}</div>
            
            <h2>üìä Prediction</h2>
            <div class="prediction">
                <p>Normal: {p_normal:.2%}</p>
                <div class="bar" style="width: {p_normal*100}%">{p_normal:.2%}</div>
                
                <p style="margin-top: 10px;">Attack: {p_attack:.2%}</p>
                <div class="bar attack" style="width: {p_attack*100}%">{p_attack:.2%}</div>
            </div>
            
            <div class="legend">
                <h3>Legend:</h3>
                <span style="background: #f44336; color: white;">ƒê·ªè = Nguy hi·ªÉm</span>
                <span style="background: #4CAF50; color: white;">Xanh = An to√†n</span>
                <span style="background: #f0f0f0;">Tr·∫Øng = Trung t√≠nh</span>
            </div>
        </div>
    </body>
    </html>
    """
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(html)
    
    print(f"\nüíæ ƒê√£ l∆∞u explanation ra file: {filename}")

# ==============================================================================
# MAIN EXPLANATION FUNCTION
# ==============================================================================
def explain_payload(payload, num_samples=1000, export_html=False):
    """
    Gi·∫£i th√≠ch t·∫°i sao payload ƒë∆∞·ª£c d·ª± ƒëo√°n l√† Attack/Normal
    
    Args:
        payload: Chu·ªói c·∫ßn ph√¢n t√≠ch
        num_samples: S·ªë l∆∞·ª£ng perturbed samples cho LIME (c√†ng nhi·ªÅu c√†ng ch√≠nh x√°c)
        export_html: C√≥ xu·∫•t file HTML kh√¥ng
    """
    # 1. T·∫°o explainer
    explainer = create_char_level_explainer()
    
    # 2. Explain
    print(f"\n‚è≥ ƒêang ph√¢n t√≠ch payload (num_samples={num_samples})...")
    exp = explainer.explain_instance(
        payload, 
        predict_proba, 
        num_features=len(set(payload)),  # Explain t·∫•t c·∫£ unique characters
        num_samples=num_samples
    )
    
    # 3. L·∫•y prediction
    probs = predict_proba([payload])[0]
    
    # 4. In k·∫øt qu·∫£
    print_explanation_summary(exp, payload, probs)
    
    # 5. N-gram analysis
    analyze_ngrams(payload, exp, n=3)
    analyze_ngrams(payload, exp, n=5)
    
    # 6. Export HTML (optional)
    if export_html:
        export_html_explanation(exp, payload, probs)
    
    return exp, probs

# ==============================================================================
# INTERACTIVE MODE
# ==============================================================================
if __name__ == "__main__":
    print("="*70)
    print("üîç WAF PAYLOAD EXPLAINER - Character-Level Analysis")
    print("="*70)
    print("\nNh·∫≠p 'exit' ƒë·ªÉ tho√°t")
    print("Nh·∫≠p 'html' sau payload ƒë·ªÉ export HTML")
    print("V√≠ d·ª•: admin' OR 1=1 -- html")
    print("-"*70)
    
    # Test v·ªõi m·ªôt s·ªë payload m·∫´u
    test_payloads = [
        "admin' OR 1=1 --",
        "<script>alert(1)</script>",
        "http://localhost:8000/api/users?id=123",
        "'; DROP TABLE users--",
        "normal_user_search_query",
    ]
    
    print("\nüéØ DEMO: Ph√¢n t√≠ch m·ªôt s·ªë payload m·∫´u")
    print("B·∫°n c√≥ mu·ªën xem demo kh√¥ng? (y/n): ", end='')
    choice = input().strip().lower()
    
    if choice == 'y':
        for payload in test_payloads:
            print(f"\n{'='*70}")
            print(f"Analyzing: {payload}")
            explain_payload(payload, num_samples=500)
            input("\nNh·∫•n Enter ƒë·ªÉ ti·∫øp t·ª•c...")
    
    # Interactive loop
    print("\n" + "="*70)
    print("üîÑ INTERACTIVE MODE")
    print("="*70)
    
    while True:
        print("\n" + "-"*70)
        payload_input = input("\nüìù Nh·∫≠p payload (ho·∫∑c 'exit' ƒë·ªÉ tho√°t): ").strip()
        
        if payload_input.lower() == 'exit':
            print("üëã T·∫°m bi·ªát!")
            break
        
        if not payload_input:
            print("‚ö†Ô∏è  Vui l√≤ng nh·∫≠p payload!")
            continue
        
        # Check if user wants HTML export
        export_html = False
        if payload_input.endswith(' html'):
            export_html = True
            payload_input = payload_input[:-5].strip()
        
        try:
            explain_payload(payload_input, num_samples=1000, export_html=export_html)
        except Exception as e:
            print(f"‚ùå L·ªói: {e}")
            import traceback
            traceback.print_exc()